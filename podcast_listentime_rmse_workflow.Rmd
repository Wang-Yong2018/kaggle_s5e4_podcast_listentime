---
title: "ml_workflow"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Target
The goal of this competition is to predict listening time of a podcast episode.

ML tools: 
  tidymodels related worksflows & glm, lightgbm, ranger engine.
  
Evaluatioin metric:
  Root Mean Squared Error (RMSE)
  
  sample_submission.csv : 27.16
  basic lm score on rcp_base_v0 : 26.86687	
  kaggle best score is 12.310 in the begining.
  after remove all nominal variable in recipe, 
    the rmse dropped to 13.45 for linear regression(lm.glm,glmnet), other models score is similar to 13.4.
    
without od 
wflow_id        model .metric mean    n std_err
bs_glmnet	linear_reg	rmse	13.37632	3	0.03552270
bs_lgbm	boost_tree	  rmse	13.15716	3	0.03250658
bs_lm	linear_reg	    rmse	13.37546	3	0.03564681
v1_glmnet	linear_reg	rmse	13.35012	3	0.03671508
v1_lgbm	boost_tree	rmse	13.14977	3	0.03586444
v1_lm	linear_reg	rmse	13.34911	3	

withod reduced the sd
bs_glmnet	linear_reg	rmse	13.37922	3	0.02340536
bs_lgbm	boost_tree	rmse	13.22553	3	0.01676875
bs_lm	linear_reg	rmse	13.37832	3	0.02334031
v1_glmnet	linear_reg	rmse	13.34808	3	0.02244264
v1_lgbm	boost_tree	rmse	13.19260	3	0.01781949
v1_lm	linear_reg	rmse	13.34688	3	0.02198646

  
Notice:


## librar y & load_data

### library

```{r}
library(tidyverse)
library(tidymodels)
library(finetune)
library(future)
library(purrr)
library(furrr)
library(textrecipes)
library(themis)
library(embed)
#library(tailor)


library(bonsai)
library(lightgbm)
library(xgboost)
library(ranger)
library(betareg)
library(readr)
library(janitor)
library(lubridate)
library(betareg)

library(memoise)
dk_cach <- memoise::cache_filesystem('./cache')
```
### download to local
```{r}
competition_name <- 'playground-series-s5e4'
data_path <- file.path('../input',competition_name)
target_name <- 'listening_time_minutes'
# system(paste0('kaggle competitions  download -c ', competition_name))
# unzip(paste0(competition_name,'.zip'),exdir=file.path('../input',competition_name))
# file.remove(paste0(competition_name,'.zip'))
```


### loading data

```{r}

train<- 
  readr::read_csv(file.path(data_path, 'train.csv'),
                  show_col_types = FALSE)|>
  janitor::clean_names()
test <- 
  readr::read_csv(file.path(data_path, 'test.csv'),
                  show_col_types = FALSE)|>
   janitor::clean_names()
submission <-  readr::read_csv(file.path(data_path, 'sample_submission.csv'),show_col_types = FALSE)
```

### quick skim

```{r}
train|> skimr::skim()
```
important notice: 
numeric variable missing
- around 12% of episode_length_minutes data missing
- around 20% of guest_popularity_percent data missing
- 1 number of ads missing
```{r}
test|> skimr::skim()
```
The missing ratio is similar as train dataset
```{r}
submission |> skimr::skim()
```

### check if train & test is same distribution

```{r}
get_df_var<-function(df){
  df|>
    dplyr::select(-any_of(c('id','listening_time_minutes')))|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')

}
list(train=train, test=test)|>
  map_dfr(\(x) get_df_var(x), .id = "dataset") |>
  pivot_wider(names_from=dataset, values_from = variance)|>
  mutate(pct_change=(train-test)/train)#|>arrange(desc(abs(diff)))
```

### Finding of different distribution
there is no big change found 

## EDA
### helpers
#### numeric plot
```{r}
library(patchwork)
plot_num_column_eda<-function(df,col_name, y_name=target_name){

  p1 <- df|>
    ggplot(aes(x=.data[[col_name]])) +
    geom_density() +theme_classic()
  
  p2 <- df|>
    ggplot(aes(x=.data[[col_name]],y=.data[[y_name]])) +
    geom_smooth() +
    theme_classic()
  p1/p2 
}

plot_cat_column_eda <- function(df, col_name,n_lump=10){

  p1 <- df|>ggplot(aes(y=.data[[col_name]]))  + geom_bar() +theme_classic()
  p2 <- df|>ggplot(aes(y=.data[[col_name]],x=.data[[target_name]])) + geom_boxplot() +theme_classic()
  p1/p2
}

plot_column_eda <- function(df, col_name, y_name=target_name){
  
  is_numeric <- is.numeric(df[[col_name]])
  if(is_numeric){
    plt <- plot_num_column_eda(df, col_name, y_name)
  }else{
    
    plt <- plot_cat_column_eda(df, col_name, y_name)
  }
  return(plt)
}

```

### check the misssing value by naniar package
#### all variables
```{r}
check_all_missing<-function(df){
  library(naniar)
  gg_miss_var(df) + labs(title = "Missing Data by Variable")
}
check_all_missing(train)

```

#### check the relationship between guest popularity and episode length
```{r}
### find the missing based on the group of podcast_name

train |>
  filter(number_of_ads<=5)|>
  mutate(na_length=is.na(episode_length_minutes),
         na_guest= is.na(guest_popularity_percentage),
         number_of_ads = as_factor(number_of_ads))|>
  group_by(podcast_name)|>
  nest() |>
  #head(10) |>
   mutate(
     test_result = map(data, 
                       ~ chisq_test(.x, response =na_guest, explanatory = number_of_ads))
   ) |>
   unnest(test_result)|>
  print()

train |>
  filter(number_of_ads<=5)|>
  mutate(na_length=is.na(episode_length_minutes),
         na_guest= is.na(guest_popularity_percentage),
         number_of_ads = as_factor(number_of_ads))|>
  group_by(podcast_name)|>
  nest() |>
  #head(10) |>
   mutate(
     test_result = map(data, 
                       ~ chisq_test(.x, response =na_guest, explanatory = number_of_ads))
   ) |>
  unnest(test_result)|>
  ggplot(aes(x = podcast_name, y =-log10(p_value), color = podcast_name)) +
 # ggplot(results, aes(x = group_var, y = -log10(p_value), fill = group_var)) +
  geom_col(width = 0.6, color = "black") +                          # 柱状图
  # geom_text(aes(label = p_value), vjust = -0.5, size = 5) +    # 添加显著性标记
   geom_hline(yintercept = -log10(0.05), linetype = "dashed",color='red') +      # 显著性阈值线
   labs(
     x = "podcast_name", 
     y = "-log10(p-value)", 
     title = "missing data reason Chisq Test Results with number_ads "
   ) +
   theme_minimal(base_size = 14) +
   scale_fill_brewer(palette = "Set2")

```


### see the listening_time_minutes distribution
```{r}
col_name <- 'listening_time_minutes'

train|>ggplot(aes(x=.data[[col_name]])) +geom_density()


kurtosis_value <- moments::kurtosis(train[[col_name]])
print(paste("样本峰度:", kurtosis_value))
##aic_normal <- AIC(MASS::fitdistr(train[[col_name]],                                 "normal"))

#aic_unif <- AIC(MASS::fitdistr(train[[col_name]], "uniform"))

#data.frame(Distribution=c("Normal", "t (df=3)", "Uniform"), AIC=c(aic_normal, aic_t, aic_unif))



```
#### check distribution
```{r}
compare_distribution<-function(){
  tmp_y <- train |> 
    mutate(tmp_y = listening_time_minutes+1) |>
    pull(tmp_y)
  
  norm_fit <- MASS::fitdistr(tmp_y, 'normal')
  lognorm_fit <- MASS::fitdistr(1+tmp_y, 'lognormal')
  #beta_fit <- MASS::fitdistr(rescale(tmp_y,to=c(0,1)),'',shape1=1.2, shape2=2.3)
  AIC(norm_fit, lognorm_fit)
}
compare_distribution()
```


### check the predict with response
#### podcast_name
```{r}
col_name <- 'podcast_name'
# train|>
#   ggplot(aes(x=podcast_name,y=listening_time_minutes)) +
#   geom_col()
# train|>count(podcast_name,wt=listening_time_minutes,sort=TRUE)

  train |> 
   mutate(
    podcast_name = case_when(
      podcast_name %in% c("Fitness First", "Health Hour", "Healthy Living", 
                         "Mind & Body", "Wellness Wave", "Life Lessons") ~ "Health & Wellness",
      podcast_name %in% c("Business Briefs", "Business Insights", "Finance Focus",
                         "Market Masters", "Money Matters", "Innovators") ~ "Business & Finance",
      podcast_name %in% c("Current Affairs", "Daily Digest", "Global News",
                         "News Roundup", "World Watch", "Digital Digest") ~ "News & Current Affairs",
      podcast_name %in% c("Comedy Corner", "Funny Folks", "Humor Hub",
                         "Joke Junction", "Laugh Line") ~ "Comedy & Entertainment",
      podcast_name %in% c("Gadget Geek", "Tech Talks", "Tech Trends",
                         "Style Guide", "Home & Living") ~ "Technology & Innovation",
      podcast_name %in% c("Athlete's Arena", "Game Day", "Sport Spot",
                         "Sports Central", "Sports Weekly") ~ "Sports",
      podcast_name %in% c("Brain Boost", "Educational Nuggets", "Learning Lab",
                         "Study Sessions", "Detective Diaries") ~ "Education & Learning",
      podcast_name %in% c("Fashion Forward", "Home & Living") ~ "Lifestyle & Culture",
      podcast_name %in% c("Crime Chronicles", "Criminal Minds", "Mystery Matters",
                         "True Crime Stories", "Sound Waves") ~ "Crime & Mystery",
      TRUE ~ "Other"
    )
  )|>
    plot_column_eda(col_name)




```
After transfor the type into less category with AI assistant, I still can not found any significant difference between different podcast_name
#### episode_title(***)
```{r}
original_col_name <- 'episode_title'
col_name = 'episode_id'
train|>mutate('{col_name}':=as.integer(str_replace(
  episode_title,
  'Episode ','')))|>
  plot_column_eda(col_name)
```
episode_title transformat to episode id will improve the models.
the episode_it has non-linear relationship with the listentime.

#### episode_length_minutes (***)
```{r}
col_name <- 'episode_length_minutes'
plot_column_eda(train, col_name)
test|>filter(episode_length_minutes>121) 
```
the episode_length_minutes is highly related to the listen time but it has boundary (lower and upper). that's say it is 4 and 121
#### genre 
```{r}
col_name <- 'genre'

plot_column_eda(train,col_name)
```
#### host_popularity_percentage
```{r}
col_name <- 'host_popularity_percentage'

plot_column_eda(train,col_name)
```
look the host_popularity_percentage has some non-linear relationship with listening time.

#### publication_day
```{r}
col_name='publication_day'
train|>
  mutate(publication_day = case_when(publication_day =='Monday'~1,
                                     publication_day =='Tuesday'~2,
                                     publication_day =='Wednesday'~3,
                                     publication_day =='Thursday'~4,
                                     publication_day =='Friday'~5,
                                     publication_day =='Saturday'~6,
                                     publication_day =='Sunday'~7))|>
ggplot(aes(x=publication_day, y=listening_time_minutes)) +
  geom_area()
plot_column_eda(train, col_name)
```

#### publication_time 
```{r}
col_name='publication_time'
train|>
  mutate(publication_time = case_when(publication_time =='Morning'~0,
                                          publication_time =='Afternoon'~1,
                                         publication_time =='Evening'~2,
                                         publication_time =='Night'~3,
                                         TRUE~ 4))|>
ggplot(aes(x=publication_time, y=listening_time_minutes)) +
  geom_area()
```

```{r}
col_name <-'dt'
train|>
  dplyr::select(publication_day,publication_time,listening_time_minutes)|>
  mutate(dt = paste0(publication_day,publication_time))|>
  plot_column_eda(col_name)
```

#### guest_popularity_percentage 
```{r}
col_name <- 'guest_popularity_percentage'
plot_column_eda(train,col_name)
```

#### number_of_ads (*** bad data, extremely value)
```{r}
col_name <- 'number_of_ads'
plot_column_eda(train, col_name)
test|>count(number_of_ads)
```
found both test and train has extremely value. Remove it manully
#### episode_sentiment 
```{r}
col_name <- 'episode_sentiment'
plot_column_eda(train, col_name)
```
#### listening_time_minutes
```{r}
col_name <- 'listening_time_minutes'
plot_column_eda(train, col_name)
```

### outlier detect base on train & test
```{r fig.height=3, fig.width=5}
library(applicable)
library(isotree)

internal_get_outliers <- function(od_target){
  remove_cols <- c('id','listening_time_minutes')
  od_tr <- train |> dplyr::select(-any_of(remove_cols))
  od_te <- od_target |>  dplyr::select(-any_of(remove_cols))
  if_mod <- apd_isolation(od_tr, ntrees = 50, nthreads = 1)
  od_score <- score(if_mod, od_te)
  
  return(od_score)
}

get_outliers <- memoise::memoise(internal_get_outliers,cache=dk_cach)

list(tr=train,te=test)|>
  map_dfr(\(data) get_outliers(data),
          .id='source') |>
  ggplot(aes(x=score,group=source,color=source)) +
  geom_density(alpha = 0.5)+labs(title='outliers compare')+theme_minimal()
ts_od <- train  |> get_outliers() |> bind_cols(train)

```
The above plot shoed the train & test data set almost perfrect similiar from outlier distance viewpoint. 
Thus, we can believe they are same distribution

## coding

### 1. Data Loading and Initial Exploration ----

### 2. Feature Engineering ----

-   leave it in the preprocessing recipe

### 3. Data Splitting ----

#### augment_df

#### split/cv

```{r}
set.seed(1234)
#train <- ts_df |> as_tibble() |>filter(source=='train')|>dplyr::select(-source)
#df_split <- initial_time_split(train, prop = 0.8)
od_train <- train |>get_outliers()|>dplyr::select(score_pctl)  |>bind_cols(train)
od_test <- test|>get_outliers()  |>dplyr::select(score_pctl) |>bind_cols(test)
# df_split <- initial_split(od_train, prop = 0.7, strata =listening_time_minutes )
df_split <- initial_split(od_train, prop = 0.7, strata =podcast_name)
train_set <- training(df_split)
test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,
                     v = 3,
                     repeats = 1,
                     strata = listening_time_minutes)
#cv_folds <- train_set |> sliding_period(index=date, period='year')
```

### 4. Preprocessing Recipe ----
#### 4.0 v0 base_rcp
```{r}
rcp_bs <-
  recipe(listening_time_minutes ~ ., data = train_set) |>
  update_role(id, new_role='ID')|>
  step_mutate(  episode_length_minutes  = ifelse(episode_length_minutes  >= 0 & episode_length_minutes  <= 121,
                                                episode_length_minutes , 
                                                NA),
                number_of_ads  = ifelse(number_of_ads  >= 0 & number_of_ads  <= 5,
                                                number_of_ads , 
                                                NA))

```

#### 4.0 v0 base_line
```{r}
rcp_bs_v0 <-
  rcp_bs|> #step_rm(date,year_offset)|>
  step_impute_median(all_numeric_predictors())|> 
  #step_log(listening_time_minutes, offset=1,skip=TRUE) |>
  step_rm(all_nominal())|>
  step_zv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> 
  check_missing(all_predictors())
```


#### 4.1 v1 adv + episodeid + length +
```{r}
rcp_bs_v1 <-
  rcp_bs|> 
  step_impute_median(all_numeric_predictors())|> 
  step_mutate(
    episode_id =as.integer(stringr::str_replace( episode_title, 'Episode ','')),
    length_normal = case_when(episode_length_minutes<1~-1, # too short
                              episode_length_minutes>120~1, # too long
                              .default=0),# normal length
    number_of_ads=as_factor(number_of_ads),
    noad =case_when(number_of_ads==0~1, .default=1)
    )|>
  step_bs(episode_length_minutes,
          episode_id,
          host_popularity_percentage,
          guest_popularity_percentage)|>
  step_mutate(podcast_name = case_when(
    podcast_name %in% c("Fitness First", "Health Hour", "Healthy Living", 
                        "Mind & Body", "Wellness Wave", "Life Lessons") ~ "Health & Wellness",
    podcast_name %in% c("Business Briefs", "Business Insights", "Finance Focus",
                        "Market Masters", "Money Matters", "Innovators") ~ "Business & Finance",
    podcast_name %in% c("Current Affairs", "Daily Digest", "Global News",
                        "News Roundup", "World Watch", "Digital Digest") ~ "News & Current Affairs",
    podcast_name %in% c("Comedy Corner", "Funny Folks", "Humor Hub",
                        "Joke Junction", "Laugh Line") ~ "Comedy & Entertainment",
    podcast_name %in% c("Gadget Geek", "Tech Talks", "Tech Trends",
                        "Style Guide", "Home & Living") ~ "Technology & Innovation",
    podcast_name %in% c("Athlete's Arena", "Game Day", "Sport Spot",
                        "Sports Central", "Sports Weekly") ~ "Sports",
    podcast_name %in% c("Brain Boost", "Educational Nuggets", "Learning Lab",
                        "Study Sessions", "Detective Diaries") ~ "Education & Learning",
    podcast_name %in% c("Fashion Forward", "Home & Living") ~ "Lifestyle & Culture",
    podcast_name %in% c("Crime Chronicles", "Criminal Minds", "Mystery Matters",
                        "True Crime Stories", "Sound Waves") ~ "Crime & Mystery",
    TRUE ~ "Other" ),
    podcast_name = forcats::as_factor(podcast_name))|>
  step_dummy(#genre, podcast_name,
             publication_day, publication_time, 
             episode_sentiment,one_hot = FALSE) %>%  # 虚拟编码（非满秩）
  
  #step_interact(~ starts_with("genre"):starts_with("publication_day"))|> 
  #step_interact(~ starts_with("genre"):starts_with("publication_time"))|> 
  step_rm(all_nominal())|>
  # step_bin2factor(all_logical_predictors())|>
  # step_novel(all_nominal_predictors())|>
  # step_unknown(all_nominal_predictors()) |>
  # step_other(all_nominal_predictors())|>
  # step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_zv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> 
  check_missing(all_predictors())
```

#### 4.2 v2 podcast target encoding
```{r}
rcp_bs_v2 <-
  rcp_bs|> 
  step_lencode_mixed(
    podcast_name,        # 分类变量名
    outcome = vars(listening_time_minutes)#,  # 目标变量
    #smoothing = 10       # 平滑参数（控制过拟合）
  ) |>
  step_mutate(
    episode_id =as.integer(stringr::str_replace( episode_title, 'Episode ','')))|>
  step_impute_linear(episode_length_minutes,,
                     guest_popularity_percentage,
                     impute_with = imp_vars(podcast_name,
                                            episode_id,
                                            publication_day,
                                            publication_time,))|> 
  step_impute_median(all_numeric_predictors())|>
  step_mutate(x1 = episode_length_minutes * host_popularity_percentage,
              x2 = episode_length_minutes * guest_popularity_percentage)|>
  step_mutate(
   # episode_id =as.integer(stringr::str_replace( episode_title, 'Episode ','')),
    length_normal = case_when(episode_length_minutes<1~-1, # too short
                              episode_length_minutes>120~1, # too long
                              .default=0),# normal length
    noad =case_when(number_of_ads==0~1, .default=1))|>
  #step_log(number_of_ads,offset=1) |>
  step_ns(episode_id,#guest_popularity_percentage,
          number_of_ads)|>
  step_bs(episode_length_minutes)|>
  step_dummy(#genre, podcast_name,
             publication_day, publication_time, 
             episode_sentiment,one_hot = FALSE) %>%  # 虚拟编码（非满秩）
  
  #step_interact(~ starts_with("genre"):starts_with("publication_day"))|> 
  #step_interact(~ starts_with("genre"):starts_with("publication_time"))|> 
  step_rm(all_nominal())|>
  # step_bin2factor(all_logical_predictors())|>
  # step_novel(all_nominal_predictors())|>
  # step_unknown(all_nominal_predictors()) |>
  # step_other(all_nominal_predictors())|>
  # step_dummy(all_nominal_predictors(),one_hot = TRUE) |>
  step_zv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> 
  check_missing(all_predictors())
```
##### diagnose
```{r}
#tmp_rcp<- rcp_bs_v2|>prep()|>juice()
#tmp_rcp |> glimpse()
```


#### 4.3 v3 imputate missing value based on ads
```{r}
rcp_bs_v3 <-
  rcp_bs|> #step_rm(date,year_offset)|>
  step_impute_linear(all_numeric_predictors())|> 
  #step_log(listening_time_minutes, offset=1,skip=TRUE) |>
  step_rm(all_nominal())|>
  step_zv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> 
  check_missing(all_predictors())

#tmp_rcp_v3_df <- rcp_bs_v3 |> prep()|>juice()
```

#### 4.4 v4 imputate missing value based on ads
```{r}
rcp_bs_v4 <-
  rcp_bs|> #step_rm(date,year_offset)|>
  step_mutate(
    has_guest=as.integer(is.na(guest_popularity_percentage)),
    has_length=as.integer(is.na(episode_length_minutes))
    )|>
  step_impute_median(all_numeric_predictors())|>
  #step_log(listening_time_minutes, offset=1,skip=TRUE) |>
  step_rm(all_nominal())|>
  step_zv(all_predictors())|>
  #step_corr(all_numeric_predictors())|>
  step_normalize(all_numeric_predictors())|> 
  check_missing(all_predictors())

 rcp_bs_v4 |> prep()|>juice()->tmp
```
#### 4.4 guest_popularity missing handlling

```{r}
set.seed(1234)
library(future)
library(furrr)
selected_rcps <- list(bs=rcp_bs_v0,
                      #v1=rcp_bs_v1,
                      #v2=rcp_bs_v2,
                      #v3=rcp_bs_v3,
                      v4=rcp_bs_v4)
plan(multisession,workers = 5)
#selected_rcps|>map(\(rcp_item) rcp_item|>prep()|>bake(new_data=train_set)|>summary())
plan(sequential)
```

### 5. Model Specification ----
#### beta engine
```{r}

```

other engine

```{r}

lm_eng <-linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")



glmnet_eng <- 
 linear_reg(penalty = 0.0129155,
               mixture = 0.2222222) |>  # Example penalty and mixture values
  set_engine("glmnet") |>
  set_mode("regression")    # Specify regression
glm_eng <- 
  linear_reg() |>  # Example penalty and mixture values
  set_engine("glm") |>
  set_mode("regression")    # Specify regression
lgbm_eng<-
   parsnip::boost_tree(
      trees = 300, # Number of trees
      learn_rate = 0.5,
     # tree_depth =5,
      loss_reduction = 0.1,
      stop_iter = 50,
      #sample_size = 0.9, # Added sample_size
      #tree_depth = tune(),
      #mtry = 0.5,
      min_n = 100
   ) |>
   set_mode("regression")|>
   set_engine("lightgbm",
              #metric='roc_auc', 
              #num_leaves = 20,
              #counts = FALSE,
              num_threads = 12,
              metric = "rmse",              # 优化目标
              # reg_alpha=0.01,
              # reg_lambda = 0.5,
              verbose=1) 

rf_eng<- rand_forest( trees = 300, 
                      #mtry=100, 
                      min_n=100) |>
  set_engine("ranger",num.threads=4)|>
  set_mode("regression") 

xgb_eng<- parsnip::boost_tree( trees = 300, 
                      learn_rate = 0.5,
                      loss_reduction = 0.01,
                      #sample_size = 0.8, # Added sample_size
                      #mtry=tune(),
                      min_n=100) |>
  set_engine("xgboost",num.threads=8)|>
  set_mode("regression")
#[1] "use_C5.0"             "use_cubist"           "use_earth"            "use_glmnet"           "use_kernlab_svm_poly" "use_kernlab_svm_rbf" 
#[7] "use_kknn"             "use_ranger"           "use_xgboost" 



earth_eng <-  # good model base score 0.8718
  mars() %>% 
  set_mode("regression") %>% 
  set_engine("earth") 

svm_eng <- 
  svm_rbf(
    cost = 1.714488,
    rbf_sigma = 0.001668101 ) %>% 
  set_mode("regression") 

kknn_eng <- 
  nearest_neighbor(neighbors = 5, 
                   #weight_func = tune()
                   ) %>% 
  set_mode("regression") %>% 
  set_engine("kknn") 

selected_eng <- list(
                     #glm=glm_eng,
                     glmnet=glmnet_eng,
                     #rf=rf_eng,
                     lgbm=lgbm_eng,
                     #gb=xgb_eng
                     #  #c50=c50_eng,
                     #  earth= earth_eng
                     # #kknn=kknn_eng,
                     #svm=svm_eng
                     lm=lm_eng
                     )

```

### 6. Workflow ----
#### set metrics
```{r}
rmse_metrics <- metric_set(yardstick::rmse) # main goal is roc_auc, accuracy is just for reference

get_augment <- function(wf, tr = train_set){
  
  train_pred <- predict(wf, new_data = tr)  # 替换为你的训练数据
  #train_resid <- residuals(wf, new_data = tr)
  
  augmented_data <- 
    bind_cols(tr, train_pred) |>
    mutate(.resid =  listening_time_minutes-.pred) 
    
  return(augmented_data)
  
}

diag_mod_fit <- function(wf){
 
  aug_df <- get_augment(wf)
  fit_mod <- wf|>extract_fit_engine()
  if (class(fit_mod)!='lm'){
      rmse_result <-
        aug_df|>
        yardstick::rmse(estimate = .pred,
                        truth =listening_time_minutes)
      print(rmse_result)
  } else{
    print( fit_mod |> glance())  
    print( fit_mod |> tidy())  
  }

  p1 <- 
    aug_df |>
    dplyr::select(id, .pred,actual_y=listening_time_minutes)|>
    pivot_longer(cols=-id, names_transform = list(name = as.factor) ) |>
    ggplot(aes(x = value, fill = name)) +
    geom_density(alpha = 0.5) +  # Semi-transparent densities
    labs(title = "Density Plot: Predicted vs. Actual",
       x = "Value", y = "Density", fill = "Variable") +
    theme_classic()
  p2 <-
    aug_df |>
    dplyr::select(id, .resid,actual_y=listening_time_minutes)|>
    ggplot(aes(x=actual_y, y=.resid)) +
    geom_smooth() +
    geom_area()+
    theme_classic()
  plt <- p1 /p2 
  #scale_fill_manual(values = c(".pred" = "blue", "actual_y" = "green"))  # Custom colors
  return(plt)
}
```

#### simple wflow

```{r}
set.seed(1234)

simple_wf_fit <- 
  workflow() |>
  add_recipe(rcp_bs_v4) |>
  add_model(lm_eng)|>
  #add_tailor(tailor_rng)
 # fit(train_set)
  
#simple_wf_fit |> glance()
  fit_resamples(cv_folds,
          control = control_resamples(verbose=TRUE),
           metrics=rmse_metrics)
  simple_wf_fit |> collect_metrics()
  
#diag_mod_fit(simple_wf_fit)
```

#### simple workflowset

```{r}
set.seed(1234)
library(stacks)
library(future)
#plan(multisession,workers = 12)

ctrl <- control_resamples(save_pred = TRUE, 
                          save_workflow = FALSE,
                          verbose=TRUE,
                          pkgs = c("recipes", 
                                   "parsnip",
                                   "yardstick",
                                   'lightgbm',
                                   'stringr',
                                   'forcats')  # Critical for parallel # 
                          )

wfs_result <-
  workflow_set(preproc = selected_rcps,
               models = selected_eng,
               cross=TRUE) |>
  option_add(
    control = control_stack_resamples(),
    metrics = rmse_metrics  ) |>
  workflow_map(fn='fit_resamples',
               #resamples = vfold_cv(od_train, v = 10,strata = rainfall) ,
               resamples =cv_folds,
               #metrics =rmse_metrics,
               #control = ctrl
               )
wfs_result|> 
  collect_metrics()  |>
  filter(.metric=='rmse')|>
  select(wflow_id, model, .metric,mean, n, std_err
         )
#plan(sequential)
```

### 7 stacking

```{r}
set.seed(1234)
library(future)

plan(multisession,workers = 12)

combined_fit <-
  stacks::stacks()|>
  stacks::add_candidates(wfs_result)|>
  stacks::blend_predictions(
    penalty = 10^seq(-2, 0.5, length = 20),  # Regularization
    lower_bound = 0,   # Force non-negative coefficients
    upper_bound = 120)     # Cap contributions at 100%)|>
  stacks::fit_members()

combined_fit|>
  autoplot(type = "weights")

autoplot(combined_fit)
#plan(sequential)
```

### 7. Tuning Grid ----
#### define tune helper
```{r}
get_tuned<- function(rcp, mod,tune_grid,is_plot=FALSE, eng_name='glmnet'){
  library(future)
  total_cores= 4
  # 定义调优控制选项
ctrl <- control_grid(
  verbose = TRUE,         # 显示详细信息
  allow_par = TRUE,       # 允许并行计算
  save_pred = TRUE,       # 保存预测结果
  save_workflow = TRUE,   # 保存工作流
  #parallel_over = "resamples"  # 并行计算方式
)
  plan(multisession,workers = total_cores - 4)
  tune_wf_fit <- 
    workflow() |>
    add_recipe(rcp) |>
    add_model(mod)|>
    tune_grid(resamples = cv_folds,
              grid = tune_grid,
              control = ctrl,
              metrics =rocauc_metrics )
  
  
  plan(sequential) 
  
  best_params <- select_best(tune_wf_fit, metric = "roc_auc")
  print(best_params)
  tuned_parameter <- tune_wf_fit |> collect_metrics() 
  
  if(is_plot){
    plt <- switch(eng_name,
                  'glmnet' = ggplot(data = tuned_parameter, aes(x = penalty, y = mean, color = as.factor(mixture))),
                  'svm' = ggplot(data = tuned_parameter, aes(x = cost, y = mean, color = as.factor(rbf_sigma)))
                 ) 
    plt + 
      geom_line() +
      geom_point() +
      #scale_x_log10() +  # 对 penalty 取对数
      labs(title = "parameter to  vs ROC AUC",
           x = "parameter 1 ",
           y = "ROC AUC",
           color = "parameter 2") +
      theme_minimal()
    plt 
  }
  return(tuned_parameter)
}
```

#### tune glmnet
```{r}
set.seed(1234)
glmnet_spec <- logistic_reg(penalty = tune(), mixture = tune()) %>%  # 调优 penalty 和 mixture
  set_engine("glmnet") %>%
  set_mode("regression")

glmnet_grid <- grid_regular(
  penalty(range = c(-5, -1)),  # log10(penalty) 的范围
  mixture(range = c(0, 1)),    # mixture 的范围（0: Ridge, 1: Lasso）
  levels = 10                   # 每个参数的网格点数
)
get_tuned(rcp_v13,glmnet_spec, glmnet_grid,is_plot=TRUE,eng_name='glmnet')
```

#### tune svm
```{r}
set.seed(1234)
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>%  # 调优 cost 和 rbf_sigma
  set_engine("kernlab") %>%
  set_mode("regression")
svm_grid <- grid_regular(
  cost(range = c(0, 1)),        # cost 的范围（log10 尺度）
  rbf_sigma(range = c(-5, -1)),   # rbf_sigma 的范围（log10 尺度）
  levels = 10                     # 每个参数的网格点数
)
get_tuned(rcp_v13,svm_spec, svm_grid,is_plot=TRUE, eng_name='svm')
```

#### tune lgbm
```{r}
# 定义 LightGBM 模型
lgbm_spec <- boost_tree(
  mode = "regression",  # 分类任务
  engine = "lightgbm",
  mtry = tune(),            # 随机选择的特征数量
  trees = tune(),           # 树的数量
  min_n = tune(),           # 叶子节点的最小样本数
  tree_depth = tune(),      # 树的最大深度
  learn_rate = tune(),      # 学习率
  loss_reduction = tune(),# 损失减少阈值
)  |>  set_mode("regression")|>
   set_engine("lightgbm",
              metric = "auc",              # 优化目标
              verbose=1) 

# 定义调参网格
lgbm_grid <- grid_regular(
  mtry(range = c(8, 13)),           # 特征数量范围
  trees(range = c(300, 800)),        # 树的数量范围
  min_n(range = c(10, 50)),          # 叶子节点的最小样本数范围
  tree_depth(range = c(5, 10)),     # 树的最大深度范围
  learn_rate(range = c(0.001, 0.1)), # 学习率范围
  loss_reduction(range = c(0, 0.001)),# 损失减少阈值范围
  levels = 10                        # 每个参数的网格点数
)
get_tuned(rcp_v13,lgbm_spec, lgbm_grid)
# tune_lgbm_workflow <- 
#   workflow_set(preproc = selected_rcps,
#                models = list(lgb=tune_lgbm_spec) ) 
# 
# # Tune the model using ANOVA Race
# tune_results <-  tune_lgbm_workflow |>
#   workflow_map(fn='tune_race_anova',
#                resamples=cv_folds,
#                seed=1234,
#                grid=tune_lgbm_grid,
#                metrics=rocauc_metrics,
#                control = control_race(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE) # Show progress
#                )
# tune_results|>rank_results()

```


#### tune
```{r}
# 
# glmnet_recipe <- rcp_bs_v0_wd_smote
# 
# glmnet_spec <- 
#   multinom_reg(penalty = tune(), mixture = tune()) %>% 
#   set_mode("regression") %>% 
#   set_engine("glmnet") 
# 
# glmnet_workflow <- 
#   workflow() %>% 
#   add_recipe(glmnet_recipe) %>% 
#   add_model(glmnet_spec) 
# 
# glmnet_grid <- tidyr::crossing(penalty = 10^seq(-4, -1, length.out = 20), 
#                                mixture = c(0.05, 0.2, 0.4, 0.6, 0.8, 1)) 
# 
# 
# glmnet_tune <- 
#   tune_grid(glmnet_workflow, resamples =cv_folds, grid = glmnet_grid,
#             control=control_grid(save_pred = TRUE, 
#                                  verbose = TRUE,
#                                  allow_par = F)) # Keep predictions
# 
# glmnet_tune |>show_best()
# ```
# #### tune lgbm
# ```{r}
# lgbm_recipe <- rcp_bs_v0_wd_smote 
# 
# lgbm_spec <-  
#   boost_tree(
#     trees = tune(),
#     tree_depth = tune(),
#     learn_rate = tune(),
#     mtry = tune(),
#     min_n = tune(), 
#     loss_reduction = numeric() ) |> 
#   set_engine("lightgbm",
#              max_bin=tune(),
#               # reg_lambda = tune(),  
#               # max_bin = tune(),
#               # min_sum_hessian_in_leaf = tune(),
#              #bagging_fraction = tune()
#             )|>
#   set_mode("regression")
# 
# lgbm_workflow <- 
#   workflow() |> 
#   add_recipe(lgbm_recipe) |>
#   add_model(lgbm_spec) 
# lgbm_grid <- lgbm_workflow |>
#   extract_parameter_set_dials( ) |>
#   update(
#     trees= trees(range=c(300,700)),
#     tree_depth = tree_depth(range = c(3, 8)),  # 原默认范围可能更宽，此处缩小
#     learn_rate = learn_rate(range = c(-2, -1)), # 指数范围：0.01 ~ 0.1
#     mtry = mtry(range=c(5,11)),
#     max_bin = integer(c(31,128)),
#     min_n = min_n(range = c(10, 50))
#   )|>
#   grid_space_filling(size=5)
# 
# set.seed(1234)
# lgbm_tune <-
#   tune_grid(lgbm_workflow, 
#             resamples = cv_folds, 
#             grid =lgbm_grid,
#             control=control_race(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE))
# 
# lgbm_tune |> show_best()
```

### 8. Cross-Validation ----

```{r}
# combined it with step3 data splitting
```

### 9. Tuning and Evaluation ----

```{r}
# plan(multisession,workers =2)
# cars_tune_results <- cars_workflow |>
#   tune_grid(
#     resamples = cars_folds,
#     grid = cars_grid,
#     metrics = metric_set(rmse),
#      control = control_grid(save_pred = TRUE, 
#                             verbose = TRUE,
#                             allow_par = F) # Keep predictions
#   )
#  
#  # Find best parameters
#  best_params <- cars_tune_results |>
#    select_best("rmse")
# 
#  # Finalize workflow with best parameters
#  final_workflow <- cars_workflow |>
#    finalize_workflow(best_params)
```

```{r}
# Fit the final workflow to the training data
# final_lgbm_fit <- last_fit(final_workflow,cars_split )
# final_lgbm_mod <- extract_workflow(final_lgbm_fit )
# collect_metrics(final_lgmb_mod)

# plan(sequential)

```

### 10. Evaluate on Test Set ----

```{r}
combined_test_result <- 
  test_set %>%
  bind_cols(predict(combined_fit, 
                    new_data=test_set,type='prob'))
combined_test_result|>rocauc_metrics(rainfall, .pred_1,event_level = 'second')
```

### 11. Prepare Submission ----

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 12)
final_model <- combined_fit#simple_wf_fit|>extract_workflow()
#final_model <- simple_wf_fit|>extract_workflow()
final_predictions <- final_model |>
   predict(new_data = od_test,
           type='prob') 
plan(sequential)

 # #Handle negative predictions
 # final_predictions <- final_predictions |>
 #   mutate(.pred= ifelse(.pred< 0, 0, .pred))

 # Save submission file
 submission |>
   mutate(Listening_Time_minutes =case_when(Listening_Time_minutes>0~Listening_Time_minutes,
                                            TRUE ~0)) |>
   #mutate(rainfall=final_predictions$.pred_1)|>
   
   readr::write_csv("submission.csv")
 zip('submission.csv.zip','submission.csv')
 
```

## kaggle submission



### score submit
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s5e4 -f submission.csv.zip -m "local cv3 score 13.45497 + post remove value <0 to 0"')

Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s5e4')

```

### notebook convert
```{r}
 library(rmd2jupyter)
 rmd2jupyter('podcast_listening_time.Rmd')
```
