---
title: "manymodels_oldway"
output: html_document
---


## librar y & load_data

### library

```{r}
library(tidyverse)
library(corrr)
library(rstatix)

library(tidymodels)
library(finetune)
library(future)
library(purrr)
library(furrr)
#library(textrecipes)
library(themis)
#library(embed)
#library(tailor)
library(furrr)

library(bonsai)
library(lightgbm)
#library(xgboost)
library(ranger)
#library(betareg)
library(readr)
library(janitor)
library(lubridate)

library(memoise)
dk_cach <- memoise::cache_filesystem('./cache')
```
### download to local
```{r}
competition_name <- 'playground-series-s5e4'
data_path <- file.path('../input',competition_name)
target_name <- 'listening_time_minutes'
# system(paste0('kaggle competitions  download -c ', competition_name))
# unzip(paste0(competition_name,'.zip'),exdir=file.path('../input',competition_name))
# file.remove(paste0(competition_name,'.zip'))
```


### loading data

```{r}
train <-
  readr::read_csv(file.path(data_path, "train.csv"),
    show_col_types = FALSE
  ) |>
  janitor::clean_names()
test <-
  readr::read_csv(file.path(data_path, "test.csv"),
    show_col_types = FALSE
  ) |>
  janitor::clean_names()
submission <- readr::read_csv(file.path(data_path, "sample_submission.csv"), show_col_types = FALSE)
```

### get_outlier
```{r}



internal_get_outliers <- function(od_target){
  library(applicable)
  library(isotree)
  remove_cols <- c('id','listening_time_minutes')
  od_tr <- od_target |> select(-any_of(remove_cols))
  #od_te <- od_target |>  select(-any_of(remove_cols))
  if_mod <- apd_isolation(od_tr, ntrees = 100, nthreads = 4)
  od_score <- score(if_mod, od_tr) |>select('score_pctl')
  return(od_score)
}

get_outliers <- memoise::memoise(internal_get_outliers,cache=dk_cach)

```

### data nest 
```{r}
full_tr <- train |>
  mutate(episode_id = parse_number(episode_title))|>
  #select(-episode_title)|>
  (\(x) bind_cols(x,get_outliers(od_target = x)))()|>
  nest(.by = podcast_name) 
tr <- full_tr|>head(2)

te <- test |>
  mutate(episode_id = parse_number(episode_title))|>
  select(-episode_title)|>
  nest(.by = podcast_name)
  
```



```{r}
plt_density <- function(df,col_name='listening_time_minutes'){
  
  df|>
    ggplot(aes(x=.data[[col_name]])) + geom_density(alpha=0.5)+ theme_classic()
           
}
```
#### step_mice
```{r}
library(mice)
library(recipes)
library(dplyr)


impute_with_mice <- function(data, formula, m = 5, ...) {
  imp <- mice(data = data, formula = formula, m = m, ...)
  mice::complete(imp, action = "long") %>%
    as_tibble() %>%
    select(-.imp, -.id) %>%
    pivot_wider(names_from = .var, values_from = .val)
}
step_mice_imputation <-
  function(recipe, ...,
           formula = . ~ ., m = 5, role = NA, trained = FALSE, skip = FALSE, id = rand_id("mice_impute")) {
    formula_expr <- rlang::quo_get_expr(enquo(formula))
    formula_chr <- as.character(formula_expr)

    add_step(
      recipe,
      step_mice_imputation_new(
        terms = enquos(...),
        formula_chr = formula_chr,
        m = m,
        role = role,
        trained = trained,
        skip = skip,
        id = id
      )
    )
  }

step_mice_imputation_new <-
  function(terms, formula_chr, m, role, trained, skip, id) {
    step(
      subclass = "mice_imputation",
      terms = terms,
      formula_chr = formula_chr,
      m = m,
      role = role,
      trained = trained,
      skip = skip,
      id = id
    )
  }
prep.step_mice_imputation <- function(x, training, info = NULL, ...) {
  predictor_vars <- x$recipe %>%
    dplyr::filter(role == "predictor") %>%
    dplyr::pull(variable)

  vars_to_impute <- as.character(x$terms) # Get the variable(s) to impute

  data_for_imputation <- training %>%
    dplyr::select(dplyr::all_of(c(vars_to_impute, predictor_vars)))

  imputed_data <- impute_with_mice(data = data_for_imputation, formula = as.character(rlang::expr(. ~ .)), m = x$m, ...)

  imputed_data <- imputed_data %>%
    dplyr::rename_with(~ vars_to_impute[[1]], .cols = dplyr::everything())

  structure(
    list(
      imputed_data = imputed_data,
      terms = x$terms
    ),
    class = "prepared_step_mice_imputation"
  )
}

bake.prepared_step_mice_imputation <- function(object, new_data, ...) {
  predictor_vars <- object$recipe %>%
    dplyr::filter(role == "predictor") %>%
    dplyr::pull(variable)

  vars_to_impute <- as.character(object$terms) # Get the variable(s) to impute

  data_for_imputation <- new_data %>%
    dplyr::select(dplyr::all_of(c(vars_to_impute, predictor_vars)))

  imputed_data <- impute_with_mice(data = data_for_imputation, formula = as.character(rlang::expr(. ~ .)), m = object$m, ...)

  imputed_data <- imputed_data %>%
    dplyr::rename_with(~ vars_to_impute[[1]], .cols = dplyr::everything())

  ret <- new_data %>%
    dplyr::select(-dplyr::all_of(vars_to_impute)) %>%
    dplyr::bind_cols(imputed_data)
  ret
}
```

#### get_rcp
```{r}
get_rcp <- function(df) {
  rcp <-
    recipe(listening_time_minutes ~.,
           # episode_length_minutes
           #  +host_popularity_percentage
           #  #+guest_popularity_percentage
           #  +number_of_ads
           # # #+podcast_name 
           # # +episode_title
           # # +genre
           # # +publication_day
           # # +publication_time
           # +episode_sentiment
           # +score_pctl,
           data = df) |>
    update_role(id, new_role='ID')|>
    step_mutate(episode_id =as.integer(stringr::str_replace(episode_title,'Episode ','')))|>  
    step_rm(episode_title)|>
    step_mutate(episode_length_minutes  = ifelse(episode_length_minutes  >= 0 & episode_length_minutes  <= 121,
                                                episode_length_minutes , 
                                                NA),
                number_of_ads  = ifelse(number_of_ads  >= 0 & number_of_ads  <= 5, number_of_ads ,  'NA'),
                has_guest=as.integer(is.na(guest_popularity_percentage)),
                guest_popularity_percentage = ifelse(is.na(guest_popularity_percentage),0,guest_popularity_percentage),
                is_weekend = if_else(publication_day %in% c("Saturday", "Sunday"), 1, 0),
                has_length=as.integer(is.na(episode_length_minutes)))|>
 #step_impute_linear(all_numeric_predictors(),impute_with =vars(guest_popularity_percentage,number_of_ads,has_guest,episode_id,host_popularity_percentage,score_pctl,is_weekend))|>
    # step_impute_bag(episode_length_minutes,                # 目标变量
    #                 impute_with = imp_vars(all_predictors()),  # 使用所有其他变量
    #                 trees = 50     # 设置邻居数（根据数据量调整）
    #                 ) |>
    #step_naomit(episode_length_minutes, skip = TRUE)|>
    step_impute_mode(all_nominal_predictors())|>
    step_mutate(
                ads_per_minute = number_of_ads / (episode_length_minutes + 1),
                #number_of_ads = as.integer(number_of_ads),
               epi_guest = guest_popularity_percentage /(episode_length_minutes + 1),
               epi_host = host_popularity_percentage /(episode_length_minutes + 1),
               ads_per_guest = number_of_ads / (guest_popularity_percentage + 1),
               ads_per_host = number_of_ads / (host_popularity_percentage + 1),
               score_pctl = score_pctl/(episode_length_minutes))|>
           
    ##step_impute_bag(all_numeric_predictors(),impute_with = imp_vars(number_of_ads,publication_day, publication_time, genre)) |>
  
    # Preprocessing steps (customize as needed)
    # step_naomit(all_predictors()) %>%       # Remove missing values
    
    #step_normalize(all_numeric_predictors()) %>% # Standardize numeric predictors
    step_novel(all_nominal_predictors()) |>
    step_unknown(all_nominal_predictors()) |>
    step_other(all_nominal_predictors()) |>
    step_dummy(all_nominal_predictors()) |> # One-hot encode categorical vars
    step_zv(all_predictors()) |>
    step_corr(all_predictors()) |>
    check_missing(all_predictors())
  return(rcp)
}
```

#### get_beta_mod
```{r}
get_beta_mod <- function(df){
   rmse_metrics <- metric_set(yardstick::rmse) # main goal is roc_auc, accuracy is just for reference
  set.seed(123)
  #df <- df|>bind_cols(df|>get_outliers())
  cv_folds <- vfold_cv(df, v = 3,strata = listening_time_minutes)
  
  lm_recipe  <- get_rcp(df)
  prep_df <- lm_recipe |> prep()|>juice()
  
  model <- betareg(
  listening_time_minutes ~ .,           # Formula (response ~ predictors)
  data = prep_df ,           # Dataframe
  link = "logit",        # Default link (others: "probit", "cloglog")
  link.phi = "log"       # Link for precision parameter (phi)
  )
  summary(model)           # Check coefficients & significance
  
}
```
#### fillna_with_mice
```{r}
get_mice_df <- function(df,dataset_id=1){
  library(mice)    
  imp <- mice(df, m = 5, maxit = 5, seed = 123,printFlag=F)
  fit <- with(imp, lm(episode_length_minutes~genre+
                        host_popularity_percentage+
                        guest_popularity_percentage+
                        publication_day+
                        publication_time+
                        number_of_ads+
                        episode_sentiment ))
  complete_df <- complete(imp, dataset_id) 
  return(complete_df)
}
```

#### get_mod
```{r}
get_mod <- safely(function(df,return_type='resample') {
  
  rmse_metrics <- metric_set(yardstick::rmse) # main goal is roc_auc, accuracy is just for reference

  
  # 2. Create 3-Fold Cross-Validation Splits
  set.seed(123)
  #df <- df|>bind_cols(df|>get_outliers())
  df <- df|> get_mice_df()
  cv_folds <- vfold_cv(df, v = 3,strata = listening_time_minutes)
  
  lm_recipe  <- get_rcp(df)
  
  # 3. Define Linear Regression Model 
  lm_eng <- linear_reg() %>% 
    set_engine("lm")
 
  lgbm_eng<-
   parsnip::boost_tree(
      trees =500, # Number of trees
      learn_rate = 0.05,
      tree_depth =12,
      loss_reduction = 0.1,
      stop_iter = 50,
      sample_size = 0.8, # Added sample_size
      #tree_depth = tune(),
      #mtry = 0.5,
      min_n = 50
   ) |>
   set_mode("regression")|>
   set_engine("lightgbm",
              #metric='rmse', 
              num_leaves =1024 ,
              #counts = FALSE,
              num_threads = 12,
              metric = "rmse",              # 优化目标
              # reg_alpha=0.01,
              # reg_lambda = 0.5,
              #verbose=1
              ) 

  
  # 4. Bundle into a Workflow
  lm_workflow <- workflow() %>% 
    add_recipe(lm_recipe) %>% 
    add_model(lm_eng)
  
  if (return_type == "resample") {
    #plan(multisession, workers = availableCores() - 1) 
    result <- lm_workflow |>
      fit_resamples(cv_folds,
        control = control_resamples(verbose = FALSE),
        metrics = rmse_metrics
      ) |>
      collect_metrics()
    #plan(sequential)
  } else if (return_type == "fit") {
    result <- lm_workflow |>
      fit(df)
  }

  return(result)
})
```


#### get_predict
```{r}
get_predict<- function(wf,newdata,target_type='non'){
  
  new_data <- newdata |> get_mice_df()
  
  .pred <- wf|>
    predict(newdata)
  
  if(target_type == 'log1p'){
    .pred <- .pred|> expm1()
  } else if( target_type == 'sqrt') {
    .pred <- .pred^2
  } else if(target_type =='beta') {
    .pred <- .pred |> rescale(to=c(0,120), from=c(0,1))
  }

  return(.pred)
}
```


```{r}

plan(multisession, workers = availableCores() - 1) 

mm_fit <- tr |>
  mutate(#cv_data = map(data, ~get_mod(.x),.progress = TRUE),
         model = map(data, ~get_mod(.x,return_type='fit'),.progress = TRUE,))
  #unnest(cv_data)
plan(sequential)
mm_fit|>
  mutate(  result = map(model, "result"),
    error = map(model, "error"))|>
  mutate(glance=map(result, ~broom::glance(.x)))|>unnest(glance)
mm_fit |>saveRDS('many_model.Rds'                 )
```

# diagnose
```{r}
library(ggfortify)
library(patchwork)
#mm_fit |> unnest(data ) |> select(podcast_name, listening_time_minutes) |>nest(.by=podcast_name,.key = 'true')
mm_result <-   mm_fit|> 
  mutate(.pred = map2(.x=model,.y=data, .f=~get_predict(wf=.x, newdata = .y,target_type='none')|>pluck('.pred')))|>
  mutate(true = map(.x=data, .f=~pluck(.x, 'listening_time_minutes')))
#  rowwise()|> mutate(true = list(pluck(data, 'listening_time_minutes')))|>
#  mutate(.pred=list(get_predict(model, data, target_type = 'none'))) |>
#  rowwise()|>mutate(rmse_score = yardstick::rmse_vec(truth=true, estimate=.pred$.pred))

# tmp_mm_fit <-mm_fit|>
#   mutate(.pred = map2(.x=model,.y=data, .f=~get_predict(wf=.x, newdata = .y,target_type='none')|>pluck('.pred')))|>
#   mutate(true = map(.x=data, .f=~pluck(.x, 'listening_time_minutes')))

tmp_true <- mm_fit |> pluck('true',1)
tmp_pred <- mm_fit |> pluck('.pred',1,1)
tmp_target <- tibble("true"=tmp_true, "pred"=tmp_pred)
tmp_target |> 
  pivot_longer(cols=everything())|>
  ggplot(aes(x=value,color=name))+geom_density()

Dtmp_mod <- mm_fit$model[[1]] |>extract_fit_engine()
#tmp_mod |> tidy()
#tmp_mod|>glance()
#mm_fit|> rowwise()|>mutate(true = list(select(data, listening_time_minutes))) |> mutate(.pred=list(get_predict(model, data))) 
#tmp_mod |>autoplot()
```

## betaregression check
### compare the response variable distribution
```{r}

library(betareg)
library(fitdistrplus)

get_fitdist_info<-function(df, col_name='listening_time_minutes'){
  
 original_y <- df[[col_name]]
 y <- original_y  / 120 +0.001/120  # 缩放到 [0,1]
 stringr::str_glue('the lisenting time value range is {min(original_y)}--{max(original_y)} (minutes)')

 # (1) 正态分布拟合
 fit_normal <- fitdist(y, "norm")

  # (2) Beta 分布（需要缩放到 0~1）
  fit_beta <- fitdist(y, "beta", method = "mme")  # 矩估计法
  
  # (3) Gamma 分布
  fit_gamma <- fitdist(y, "gamma")
  
  # (4) 对数正态分布（如果 y > 0）
   fit_lnorm <- fitdist(y, "lnorm")
  
  # 比较AIC 
  gof_stats <- gofstat(list(fit_normal, fit_beta, fit_gamma, fit_lnorm),
                     fitnames = c("Normal", "Beta", "Gamma", "Lognormal"))
   print(gof_stats)
  # par(mfrow = c(2, 2))
  # plot(fit_normal, main = "Normal")
  # plot(fit_beta, main = "Beta")
  # plot(fit_gamma, main = "Gamma")
  # plot(fit_lnorm, main = "Lognormal")
   
}


get_fitdist_info()
```


# predict
```{r}
mm_fit |> ggplot(aes(x = sigma, y = adj.r.squared)) +
  geom_jitter()

te <- test |> head()
nest(.by = podcast_name) |>
  select(podcast_name, te_data = data)
combined <- left_join(te, mm_fit, by = "podcast_name")

combined |>
  head() |>
  mutate(.pred = map2(models, te_data, predict)) -> tmp_pred
tmp_pred
```


```{r}
library(embed) 
tmp_df <- tr|>dplyr::slice(1)|>unnest(data)|>mutate(episode_id =as.integer(stringr::str_replace( episode_title, 'Episode ','')))

lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")


lm_rcp <- 
  recipe(listening_time_minutes ~ episode_length_minutes+guest_popularity_percentage+episode_id, data = tmp_df)|>
#  step_mutate( episode_id =as.integer(stringr::str_replace( episode_title, 'Episode ','')))|>  
  step_mutate(guest_popularity_percentage=replace_na(guest_popularity_percentage,0 ),
              has_guest=ifelse(is.na(guest_popularity_percentage),0,1))|>
  step_impute_linear(episode_length_minutes,impute_with = imp_vars(episode_id))|>
  step_rm(all_nominal_predictors())|>
  check_missing(all_predictors())

lm_wf <- workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(lm_rcp)


lm_fit <- lm_wf %>%
  fit(tmp_df)
lm_fit|>glance()
lm_fit|>tidy()
```



## focus on Mystery Matters case
```{r}

tmp_input <- full_tr |>
  dplyr::slice(1)|>
  unnest(data) |>
  get_mice_df()
  #filter(episode_id ==80)


tmp_mod <- tmp_input |> get_mod(return_type = 'fit')
tmp_mod|>glance()
tmp_output <- tmp_mod |> augment(tmp_input) 
tmp_output|>head()
```
#### missing episode_length lead to high residual
##### lm diagnose
```{r}

c(1:5)|>
  map(\(x) tmp_mod |> extract_fit_engine()|> plot(x=_, x))

```
```{r}

```

##### residual plot with if missing.
as the lm diagnose plot has strong points in the middel. plot is with color if missing episode_length_minutes.

```{r}
tmp_output|>
  mutate(has_length=!is.na(episode_length_minutes))|>
  t_test(.resid~episode_length_minutes, na)
```

```{r}
tmp_output |>
  mutate(missing_data=is.na(episode_length_minutes))|>
  ggplot(aes(x=.pred, y =.resid,color=missing_data)) +
  geom_jitter(alpha=0.9) + 
  labs(title='residual deviation vs missing pod length(minutes0')+
  theme_classic()
```

finding :
1. missing data of episode length lead to high residual deviation. need find better solution
2. with the episode id gain, the residual deviation gain. need find better solution .

##### qq plot
```{r}
tmp_output|>
ggplot(aes(sample = .resid)) +
  geom_qq() +
  geom_qq_line(color = "red") +
  labs(title = "Normal Q-Q Plot", x = "Theoretical Quantiles", y = "Standardized Residuals")
```

#####  scaled plot
```{r}
tmp_output|>
  ggplot(aes(.pred, sqrt(abs(.resid)))) +
  geom_point() +
  geom_smooth(se = FALSE, method = "loess", color = "red") +
  labs(title = "Scale-Location Plot", x = "Fitted Values", y = "sqrt(|Standardized Residuals|)")

```

##### residual vs leverage 
```{r}
tmp_output|>
ggplot(aes(listening_time_minutes, .resid)) +
  geom_point(aes(size = .cooksd), alpha = 0.6) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Residuals vs Leverage", x = "Leverage", y = "Standardized Residuals") +
  scale_size_continuous("Cook's Distance", range = c(1, 5))
```
##### lag & lead
##### original
```{r}
df_1<- tmp_input|>lm(episode_length_minutes~ genre+ host_popularity_percentage + publication_day+publication_time+ guest_popularity_percentage +number_of_ads+episode_sentiment+listening_time_minutes+episode_id,data=_) |>glance()|>arrange(p.value)

df_2 <- tmp_input|>lm(episode_length_minutes~ genre+ host_popularity_percentage + publication_day+publication_time+ guest_popularity_percentage +number_of_ads+episode_sentiment+episode_id,data=_) |>glance()|>arrange(p.value)
bind_rows(wi_listening_time = df_1, wo_listening_time=df_2,.id = 'source')
```

```{r}
tmp_input |>mutate(lag_length=lag(episode_length_minutes,order_by = episode_id),
                   lead_length=lead(episode_length_minutes,order_by=episode_id))|>
  arrange(episode_id)|>
  select(episode_id,episode_length_minutes, lag_length, lead_length)
```



